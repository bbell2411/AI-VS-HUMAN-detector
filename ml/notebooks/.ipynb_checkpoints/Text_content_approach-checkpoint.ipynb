{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "fb216e76-5d02-4d18-9d5b-13ffe514e28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "06a0bc10-4ea9-40e4-a2c4-c9bdcfb89cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Load data\n",
    "df = pd.read_csv('../data/ai_human_content_detection_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "06337d4d-3d27-4d40-9362-190ed08d56b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables for results and evaluation\n",
    "results={}\n",
    "\n",
    "highest_score={\n",
    "       'model': None,\n",
    "       'score': 0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "ec719f55-e70d-4496-af60-6c0c3ca6c6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models \n",
    "models = {\n",
    "    'RandomForest': RandomForestClassifier(random_state=42),\n",
    "    'LogisticRegression': LogisticRegression(random_state=42, max_iter=100000),\n",
    "    'SVM': SVC(random_state=42),\n",
    "    'KNN': KNeighborsClassifier()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "6b3b2150-6dcd-4423-a455-d2e9b2e9b2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X data with raw numeric-only values\n",
    "X_numeric=df[['word_count', 'character_count',\n",
    "       'sentence_count', 'lexical_diversity', 'avg_sentence_length',\n",
    "       'avg_word_length', 'punctuation_ratio', 'flesch_reading_ease',\n",
    "       'gunning_fog_index', 'grammar_errors', 'passive_voice_ratio',\n",
    "       'predictability_score', 'burstiness', 'sentiment_score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "cc8718d1-d8fd-4502-aac3-e542afb076dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorised text_content values (essay/assignment free text) with modification\n",
    "vectorizer = TfidfVectorizer(  \n",
    "    max_features=950,  \n",
    "    ngram_range=(1, 3),  # Include bigrams and trigrams\n",
    "    min_df=2,  # Ignore very rare words\n",
    "    max_df=0.9,  # Ignore very common words\n",
    "    sublinear_tf=True,  # Apply sublinear scaling\n",
    "    use_idf=True,\n",
    "    smooth_idf=True\n",
    "    )  \n",
    "X_text = vectorizer.fit_transform(df['text_content'])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "a3d3a9b8-9dae-4fe9-b086-5fbda4c39be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding simple one-worded content_type data to numeric values\n",
    "X_cat = pd.get_dummies(df['content_type'], drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "2853d22f-0fed-4290-b6a0-14afdb99ff38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining all numeric and encoded numeric values in one sparse array\n",
    "X_non_text = np.hstack([X_numeric.values, X_cat.values])\n",
    "X_full = hstack([X_non_text, X_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "9639a612-333b-4b7a-bad3-4156c9df6f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y value\n",
    "y=df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "b4ddaafd-e299-429e-9184-fb852094a062",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/Test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "       X_full,\n",
    "       y,\n",
    "       test_size=0.2,\n",
    "       random_state=42,\n",
    "       stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "ab3862d3-bd39-42b7-a09c-14fc79778420",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute nan/missing values\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_train_imputed = imputer.fit_transform(X_train)\n",
    "X_test_imputed = imputer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "00b5335f-10c6-4f69-9855-b297dfbd941b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAW MODEL EVAL: RandomForest: 0.511\n",
      "RAW MODEL EVAL: LogisticRegression: 0.573\n",
      "RAW MODEL EVAL: SVM: 0.493\n",
      "RAW MODEL EVAL: KNN: 0.529\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate all models (no scaling or PCA)\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train_imputed, y_train)\n",
    "    y_pred = model.predict(X_test_imputed)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    results[name] = accuracy\n",
    "    print(f\"RAW MODEL EVAL: {name}: {accuracy:.3f}\")\n",
    "best_model = max(results, key=results.get)\n",
    "\n",
    "# Store highest score\n",
    "if results[best_model] > highest_score[\"score\"]:\n",
    "       highest_score[\"score\"] = results[best_model]\n",
    "       highest_score[\"model\"] = best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "4763252e-5bf9-469d-841a-e12b6e763d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale training and test data\n",
    "scaler = StandardScaler(with_mean=False)  # with_mean=False for sparse matrices\n",
    "X_train_scaled = scaler.fit_transform(X_train_imputed) #fit and transform only on training data\n",
    "X_test_scaled= scaler.transform(X_test_imputed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "c64d35c8-9baf-44e6-bed4-99be0812aa07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert sparse matrices to dense (FOR PCA)\n",
    "X_train_scaled_dense = X_train_scaled.toarray()  \n",
    "X_test_scaled_dense = X_test_scaled.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "6b5e6018-ad2d-4297-bbf0-ff619d500739",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCALED MODEL EVAL (DENSE): RandomForest: 0.511\n",
      "SCALED MODEL EVAL (DENSE): LogisticRegression: 0.526\n",
      "SCALED MODEL EVAL (DENSE): SVM: 0.591\n",
      "SCALED MODEL EVAL (DENSE): KNN: 0.493\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate all models with scaled data\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train_scaled_dense, y_train)\n",
    "    y_pred = model.predict(X_test_scaled_dense)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    results[name] = accuracy\n",
    "    print(f\"SCALED MODEL EVAL (DENSE): {name}: {accuracy:.3f}\")\n",
    "best_model = max(results, key=results.get)\n",
    "\n",
    "# Store highest score\n",
    "if results[best_model] > highest_score[\"score\"]:\n",
    "       highest_score[\"score\"] = results[best_model]\n",
    "       highest_score[\"model\"] = best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "773a5c39-c0d9-44eb-8fff-d4d9e2d0d0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA to capture 95% variance\n",
    "pca=PCA(n_components=0.95)\n",
    "X_train_pca = pca.fit_transform(X_train_scaled_dense)\n",
    "X_test_pca = pca.transform(X_test_scaled_dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "9c69420d-1411-47cf-b6e6-9235e630d455",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA APPLIED MODEL EVAL RandomForest: 0.507\n",
      "PCA APPLIED MODEL EVAL LogisticRegression: 0.547\n",
      "PCA APPLIED MODEL EVAL SVM: 0.602\n",
      "PCA APPLIED MODEL EVAL KNN: 0.507\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate all models with scaled + PCA data\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train_pca, y_train)\n",
    "    y_pred = model.predict(X_test_pca)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    results[name] = accuracy\n",
    "    print(f\"PCA APPLIED MODEL EVAL {name}: {accuracy:.3f}\")\n",
    "    \n",
    "# Store highest score\n",
    "if results[best_model] > highest_score[\"score\"]:\n",
    "       highest_score[\"score\"] = results[best_model]\n",
    "       highest_score[\"model\"] = best_model\n",
    "best_model = max(results, key=results.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "91605325-41d6-4f89-8453-1f55cb1f2314",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === EXPERIMENT: Optimizing LogisticRegression C parameter ===\n",
    "# After finding LogisticRegression was initially best, tested different regularization strengths\n",
    "# RESULT: Default C=1.0 was optimal for 100 features, but with 600 features, C=0.1 performed better\n",
    "# This led to discovering that more text features (600 vs 100) was more impactful than C tuning\n",
    "lr_models = {\n",
    "    'LR_C=0.1': LogisticRegression(C=0.1, random_state=42, max_iter=10000),\n",
    "    'LR_C=1.0': LogisticRegression(C=1.0, random_state=42, max_iter=10000), \n",
    "    'LR_C=10.0': LogisticRegression(C=10.0, random_state=42, max_iter=10000),\n",
    "}\n",
    "\n",
    "for name, model in lr_models.items():\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    accuracy = accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "0333a6bd-3331-44e6-af63-0c043fa819c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best RAW Model: SVM with accuracy: 0.602\n"
     ]
    }
   ],
   "source": [
    "print(f\"Best RAW Model: {best_model} with accuracy: {results[best_model]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "1e75fecf-49f7-4671-b877-68a5f4e4a225",
   "metadata": {},
   "outputs": [],
   "source": [
    "# THOUGHT PROCESS AND EVAL:\n",
    "# Grid search with cross-validation and multiple C values worsened score and same with linearSVC\n",
    "# optimised second best model (lr) with best .54 lower than svm\n",
    "# optimised nbm model scored second best optimised at 5.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "f7d00742-ab57-4c5f-a4c9-ca2a748d18a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Model\n",
    "best_svm = SVC(C=1, gamma='scale', kernel='rbf')\n",
    "best_svm.fit(X_train_pca, y_train)\n",
    "y_pred_final = best_svm.predict(X_test_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "32f40129-c3db-43ec-a3c0-19cde7130550",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Model Performance:\n",
      "Accuracy: 0.602\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.58      0.59       137\n",
      "           1       0.60      0.63      0.61       137\n",
      "\n",
      "    accuracy                           0.60       274\n",
      "   macro avg       0.60      0.60      0.60       274\n",
      "weighted avg       0.60      0.60      0.60       274\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[79 58]\n",
      " [51 86]]\n"
     ]
    }
   ],
   "source": [
    "# Performance\n",
    "print(\"Final Model Performance:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_final):.3f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_final))\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "d2d0248d-87c7-49aa-b5e8-1ecb5309c9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TESTING WITH NMB MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "2cf20603-3b97-4005-8609-273fbf2b0694",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NMB model fail\n",
    "nb = MultinomialNB()\n",
    "\n",
    "# Make all values positive by adding the minimum\n",
    "X_train_positive = X_train_imputed.toarray() - X_train_imputed.toarray().min()\n",
    "X_test_positive = X_test_imputed.toarray() - X_test_imputed.toarray().min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "c8ab2f8f-b3f3-4b9d-8cb0-bfa5f9d263e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NBM RAW: 0.49635036496350365\n"
     ]
    }
   ],
   "source": [
    "nb.fit(X_train_positive, y_train)  \n",
    "y_pred_nmb = nb.predict(X_test_positive)\n",
    "accuracy = accuracy_score(y_test, y_pred_nmb)\n",
    "print(f\"NBM RAW: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "f50cd9ea-289b-4fc5-a677-bb0eabee8755",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This ensures values stay between 0 and 1\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_positive)\n",
    "X_test_scaled = scaler.transform(X_test_positive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "444419d3-6d81-4068-b346-0f06d297cd9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NBM SCALED: 0.5\n"
     ]
    }
   ],
   "source": [
    "nb.fit(X_train_scaled, y_train)\n",
    "y_pred_scaled= nb.predict(X_test_scaled)\n",
    "accuracy = accuracy_score(y_test, y_pred_scaled)\n",
    "print(f\"NBM SCALED: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "82f13625-9c17-494c-b7fa-6e9b41579f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA\n",
    "pca=PCA(n_components=0.95)\n",
    "X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "X_test_pca = pca.transform(X_test_scaled)\n",
    "# After PCA, shift to positive\n",
    "X_train_pca_positive = X_train_pca - X_train_pca.min()\n",
    "X_test_pca_positive = X_test_pca - X_test_pca.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "b3ad2ea0-cfe0-43a7-86ac-bacb781dc355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NBM PCA: 0.5\n"
     ]
    }
   ],
   "source": [
    "nb.fit(X_train_pca_positive, y_train)\n",
    "y_pred_pca = nb.predict(X_test_pca_positive)\n",
    "accuracy = accuracy_score(y_test, y_pred_pca)\n",
    "print(f\"NBM PCA: {accuracy}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
