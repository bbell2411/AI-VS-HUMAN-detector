{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# FULL FEATURE\n",
    "\n",
    "Quick experiment to test if both text and numeric features (full feature) can distinguish AI vs Human writing\n",
    "\n",
    "**Result**: ~60.2% with full features vs 55% accuracy with numeric-only feature approach\n",
    "\n",
    "**Conclusion**: The full features approach provides meaningful improvement over numeric-only feature approach\n",
    "\n",
    "*Note: This was an exploratory experiment. See main.py for the complete, production-ready pipeline.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Load data\n",
    "df = pd.read_csv('../data/ai_human_content_detection_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables for results and evaluation\n",
    "results={}\n",
    "\n",
    "highest_score={\n",
    "       'model': None,\n",
    "       'score': 0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models \n",
    "models = {\n",
    "    'RandomForest': RandomForestClassifier(random_state=42),\n",
    "    'LogisticRegression': LogisticRegression(random_state=42, max_iter=100000),\n",
    "    'SVM': SVC(random_state=42),\n",
    "    'KNN': KNeighborsClassifier()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X data with raw numeric-only values\n",
    "X_numeric=df[['word_count', 'character_count',\n",
    "       'sentence_count', 'lexical_diversity', 'avg_sentence_length',\n",
    "       'avg_word_length', 'punctuation_ratio', 'flesch_reading_ease',\n",
    "       'gunning_fog_index', 'grammar_errors', 'passive_voice_ratio',\n",
    "       'predictability_score', 'burstiness', 'sentiment_score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorised text_content values (essay/assignment free text) with modification\n",
    "vectorizer = TfidfVectorizer(  \n",
    "    max_features=950,  \n",
    "    ngram_range=(1, 3),  # Include bigrams and trigrams\n",
    "    min_df=2,  # Ignore very rare words\n",
    "    max_df=0.9,  # Ignore very common words\n",
    "    sublinear_tf=True,  # Apply sublinear scaling\n",
    "    use_idf=True,\n",
    "    smooth_idf=True\n",
    "    )  \n",
    "X_text = vectorizer.fit_transform(df['text_content'])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding simple one-worded content_type data to numeric values\n",
    "X_cat = pd.get_dummies(df['content_type'], drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining all numeric and encoded numeric values in one sparse array\n",
    "X_non_text = np.hstack([X_numeric.values, X_cat.values])\n",
    "X_full = hstack([X_non_text, X_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y value\n",
    "y=df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/Test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "       X_full,\n",
    "       y,\n",
    "       test_size=0.2,\n",
    "       random_state=42,\n",
    "       stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute nan/missing values\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_train_imputed = imputer.fit_transform(X_train)\n",
    "X_test_imputed = imputer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate all models (no scaling or PCA)\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train_imputed, y_train)\n",
    "    y_pred = model.predict(X_test_imputed)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    results[name] = accuracy\n",
    "    print(f\"RAW MODEL EVAL: {name}: {accuracy:.3f}\")\n",
    "best_model = max(results, key=results.get)\n",
    "\n",
    "# Store highest score\n",
    "if results[best_model] > highest_score[\"score\"]:\n",
    "       highest_score[\"score\"] = results[best_model]\n",
    "       highest_score[\"model\"] = best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale training and test data\n",
    "scaler = StandardScaler(with_mean=False)  # with_mean=False for sparse matrices\n",
    "X_train_scaled = scaler.fit_transform(X_train_imputed) #fit and transform only on training data\n",
    "X_test_scaled= scaler.transform(X_test_imputed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert sparse matrices to dense (FOR PCA)\n",
    "X_train_scaled_dense = X_train_scaled.toarray()  \n",
    "X_test_scaled_dense = X_test_scaled.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate all models with scaled data\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train_scaled_dense, y_train)\n",
    "    y_pred = model.predict(X_test_scaled_dense)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    results[name] = accuracy\n",
    "    print(f\"SCALED MODEL EVAL (DENSE): {name}: {accuracy:.3f}\")\n",
    "best_model = max(results, key=results.get)\n",
    "\n",
    "# Store highest score\n",
    "if results[best_model] > highest_score[\"score\"]:\n",
    "       highest_score[\"score\"] = results[best_model]\n",
    "       highest_score[\"model\"] = best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA to capture 95% variance\n",
    "pca=PCA(n_components=0.95)\n",
    "X_train_pca = pca.fit_transform(X_train_scaled_dense)\n",
    "X_test_pca = pca.transform(X_test_scaled_dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate all models with scaled + PCA data\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train_pca, y_train)\n",
    "    y_pred = model.predict(X_test_pca)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    results[name] = accuracy\n",
    "    print(f\"PCA APPLIED MODEL EVAL {name}: {accuracy:.3f}\")\n",
    "    \n",
    "# Store highest score\n",
    "if results[best_model] > highest_score[\"score\"]:\n",
    "       highest_score[\"score\"] = results[best_model]\n",
    "       highest_score[\"model\"] = best_model\n",
    "best_model = max(results, key=results.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === EXPERIMENT: Optimizing LogisticRegression C parameter ===\n",
    "# After finding LogisticRegression was initially best, tested different regularization strengths\n",
    "# RESULT: Default C=1.0 was optimal for 100 features, but with 600 features, C=0.1 performed better\n",
    "# This led to discovering that more text features (600 vs 100) was more impactful than C tuning\n",
    "lr_models = {\n",
    "    'LR_C=0.1': LogisticRegression(C=0.1, random_state=42, max_iter=10000),\n",
    "    'LR_C=1.0': LogisticRegression(C=1.0, random_state=42, max_iter=10000), \n",
    "    'LR_C=10.0': LogisticRegression(C=10.0, random_state=42, max_iter=10000),\n",
    "}\n",
    "\n",
    "for name, model in lr_models.items():\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    accuracy = accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Best RAW Model: {best_model} with accuracy: {results[best_model]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# THOUGHT PROCESS AND EVAL:\n",
    "# Grid search with cross-validation and multiple C values worsened score and same with linearSVC\n",
    "# optimised second best model (lr) with best .54 lower than svm\n",
    "# optimised nbm model scored second best optimised at 5.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Model\n",
    "best_svm = SVC(C=1, gamma='scale', kernel='rbf')\n",
    "best_svm.fit(X_train_pca, y_train)\n",
    "y_pred_final = best_svm.predict(X_test_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance\n",
    "print(\"Final Model Performance:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_final):.3f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_final))\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TESTING WITH NMB MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NMB model fail\n",
    "nb = MultinomialNB()\n",
    "\n",
    "# Make all values positive by adding the minimum\n",
    "X_train_positive = X_train_imputed.toarray() - X_train_imputed.toarray().min()\n",
    "X_test_positive = X_test_imputed.toarray() - X_test_imputed.toarray().min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb.fit(X_train_positive, y_train)  \n",
    "y_pred_nmb = nb.predict(X_test_positive)\n",
    "accuracy = accuracy_score(y_test, y_pred_nmb)\n",
    "print(f\"NBM RAW: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This ensures values stay between 0 and 1\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_positive)\n",
    "X_test_scaled = scaler.transform(X_test_positive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb.fit(X_train_scaled, y_train)\n",
    "y_pred_scaled= nb.predict(X_test_scaled)\n",
    "accuracy = accuracy_score(y_test, y_pred_scaled)\n",
    "print(f\"NBM SCALED: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA\n",
    "pca=PCA(n_components=0.95)\n",
    "X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "X_test_pca = pca.transform(X_test_scaled)\n",
    "# After PCA, shift to positive\n",
    "X_train_pca_positive = X_train_pca - X_train_pca.min()\n",
    "X_test_pca_positive = X_test_pca - X_test_pca.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb.fit(X_train_pca_positive, y_train)\n",
    "y_pred_pca = nb.predict(X_test_pca_positive)\n",
    "accuracy = accuracy_score(y_test, y_pred_pca)\n",
    "print(f\"NBM PCA: {accuracy}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
