{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# NUMERIC-ONLY FEATURE\n",
    "\n",
    "Quick experiment to test if the numeric-only features available can distinguish AI vs Human writing\n",
    "\n",
    "**Result**: ~55% accuracy with numeric-only feature approach vs 60.2% with full features\n",
    "\n",
    "**Conclusion**: The full features approach provides meaningful improvement over numeric-only feature approach\n",
    "\n",
    "*Note: This was an exploratory experiment. See main.py for the complete, production-ready pipeline.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports \n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df = pd.read_csv('../data/ai_human_content_detection_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables for storing and evaluating \n",
    "results={}\n",
    "\n",
    "highest_score={\n",
    "       'model': None,\n",
    "       'score': 0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models\n",
    "models = {\n",
    "    'RandomForest': RandomForestClassifier(random_state=42),\n",
    "    'LogisticRegression': LogisticRegression(random_state=42, max_iter=100000),\n",
    "    'SVM': SVC(random_state=42),\n",
    "    'KNN': KNeighborsClassifier()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only numeric values from original data\n",
    "X_numeric=df[['word_count', 'character_count',\n",
    "       'sentence_count', 'lexical_diversity', 'avg_sentence_length',\n",
    "       'avg_word_length', 'punctuation_ratio', 'flesch_reading_ease',\n",
    "       'gunning_fog_index', 'grammar_errors', 'passive_voice_ratio',\n",
    "       'predictability_score', 'burstiness', 'sentiment_score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Desired output\n",
    "y=df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split training/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "       X_numeric,\n",
    "       y,\n",
    "       test_size=0.2,\n",
    "       random_state=42,\n",
    "       stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_train_imputed = imputer.fit_transform(X_train)\n",
    "X_test_imputed = imputer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and eval raw model\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train_imputed, y_train)\n",
    "    y_pred = model.predict(X_test_imputed)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    results[name] = accuracy\n",
    "    print(f\"RAW MODEL EVAL: {name}: {accuracy:.3f}\")\n",
    "best_model = max(results, key=results.get)\n",
    "if results[best_model] > highest_score[\"score\"]:\n",
    "       highest_score[\"score\"] = results[best_model]\n",
    "       highest_score[\"model\"] = best_model\n",
    "print(\"\")\n",
    "print(f\"Best RAW Model: {best_model} with accuracy: {results[best_model]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale\n",
    "scaler = StandardScaler(with_mean=False)  \n",
    "X_train_scaled = scaler.fit_transform(X_train_imputed) \n",
    "X_test_scaled= scaler.transform(X_test_imputed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate all models with scaled data\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    results[name] = accuracy\n",
    "    print(f\"SCALED MODEL EVAL (DENSE): {name}: {accuracy:.3f}\")\n",
    "best_model = max(results, key=results.get)\n",
    "if results[best_model] > highest_score[\"score\"]:\n",
    "       highest_score[\"score\"] = results[best_model]\n",
    "       highest_score[\"model\"] = best_model\n",
    "print(f\"Best SCALED Model: {best_model} with accuracy: {results[best_model]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA to capture 95% variance\n",
    "pca=PCA(n_components=0.95)\n",
    "X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "X_test_pca = pca.transform(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate all models with scaled + PCA data\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train_pca, y_train)\n",
    "    y_pred = model.predict(X_test_pca)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    results[name] = accuracy\n",
    "    print(f\"PCA APPLIED MODEL EVAL {name}: {accuracy:.3f}\")\n",
    "if results[best_model] > highest_score[\"score\"]:\n",
    "       highest_score[\"score\"] = results[best_model]\n",
    "       highest_score[\"model\"] = best_model\n",
    "best_model = max(results, key=results.get)\n",
    "print(f\"Best PCA Model: {best_model} with accuracy: {results[best_model]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"BEST SCORING MODEL: {highest_score['model']} with accuracy: {highest_score['score']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# performed worse without text content "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
