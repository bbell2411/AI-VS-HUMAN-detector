{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fb216e76-5d02-4d18-9d5b-13ffe514e28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "06a0bc10-4ea9-40e4-a2c4-c9bdcfb89cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Load data\n",
    "df = pd.read_csv('../data/ai_human_content_detection_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "06337d4d-3d27-4d40-9362-190ed08d56b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables for results and evaluation\n",
    "results={}\n",
    "\n",
    "highest_score={\n",
    "       'model': None,\n",
    "       'score': 0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ec719f55-e70d-4496-af60-6c0c3ca6c6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models \n",
    "models = {\n",
    "    'RandomForest': RandomForestClassifier(random_state=42),\n",
    "    'LogisticRegression': LogisticRegression(random_state=42, max_iter=100000),\n",
    "    'SVM': SVC(random_state=42),\n",
    "    'KNN': KNeighborsClassifier()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6b3b2150-6dcd-4423-a455-d2e9b2e9b2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X data with raw numeric-only values\n",
    "X_numeric=df[['word_count', 'character_count',\n",
    "       'sentence_count', 'lexical_diversity', 'avg_sentence_length',\n",
    "       'avg_word_length', 'punctuation_ratio', 'flesch_reading_ease',\n",
    "       'gunning_fog_index', 'grammar_errors', 'passive_voice_ratio',\n",
    "       'predictability_score', 'burstiness', 'sentiment_score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cc8718d1-d8fd-4502-aac3-e542afb076dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorised text_content values (essay/assignment free text) with modification\n",
    "vectorizer = TfidfVectorizer(  \n",
    "    max_features=950,  \n",
    "    ngram_range=(1, 3),  # Include bigrams and trigrams\n",
    "    min_df=2,  # Ignore very rare words\n",
    "    max_df=0.9,  # Ignore very common words\n",
    "    sublinear_tf=True,  # Apply sublinear scaling\n",
    "    use_idf=True,\n",
    "    smooth_idf=True\n",
    "    )  \n",
    "X_text = vectorizer.fit_transform(df['text_content'])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a3d3a9b8-9dae-4fe9-b086-5fbda4c39be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding simple one-worded content_type data to numeric values\n",
    "X_cat = pd.get_dummies(df['content_type'], drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2853d22f-0fed-4290-b6a0-14afdb99ff38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining all numeric and encoded numeric values in one sparse array\n",
    "X_non_text = np.hstack([X_numeric.values, X_cat.values])\n",
    "X_full = hstack([X_non_text, X_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9639a612-333b-4b7a-bad3-4156c9df6f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y value\n",
    "y=df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b4ddaafd-e299-429e-9184-fb852094a062",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/Test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "       X_full,\n",
    "       y,\n",
    "       test_size=0.2,\n",
    "       random_state=42,\n",
    "       stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ab3862d3-bd39-42b7-a09c-14fc79778420",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute nan/missing values\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_train_imputed = imputer.fit_transform(X_train)\n",
    "X_test_imputed = imputer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "00b5335f-10c6-4f69-9855-b297dfbd941b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAW MODEL EVAL: RandomForest: 0.511\n",
      "RAW MODEL EVAL: LogisticRegression: 0.573\n",
      "RAW MODEL EVAL: SVM: 0.493\n",
      "RAW MODEL EVAL: KNN: 0.529\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate all models (no scaling or PCA)\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train_imputed, y_train)\n",
    "    y_pred = model.predict(X_test_imputed)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    results[name] = accuracy\n",
    "    print(f\"RAW MODEL EVAL: {name}: {accuracy:.3f}\")\n",
    "best_model = max(results, key=results.get)\n",
    "\n",
    "# Store highest score\n",
    "if results[best_model] > highest_score[\"score\"]:\n",
    "       highest_score[\"score\"] = results[best_model]\n",
    "       highest_score[\"model\"] = best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4763252e-5bf9-469d-841a-e12b6e763d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale training and test data\n",
    "scaler = StandardScaler(with_mean=False)  # with_mean=False for sparse matrices\n",
    "X_train_scaled = scaler.fit_transform(X_train_imputed) #fit and transform only on training data\n",
    "X_test_scaled= scaler.transform(X_test_imputed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c64d35c8-9baf-44e6-bed4-99be0812aa07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert sparse matrices to dense (FOR PCA)\n",
    "X_train_scaled_dense = X_train_scaled.toarray()  \n",
    "X_test_scaled_dense = X_test_scaled.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6b5e6018-ad2d-4297-bbf0-ff619d500739",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCALED MODEL EVAL (DENSE): RandomForest: 0.511\n",
      "SCALED MODEL EVAL (DENSE): LogisticRegression: 0.526\n",
      "SCALED MODEL EVAL (DENSE): SVM: 0.591\n",
      "SCALED MODEL EVAL (DENSE): KNN: 0.493\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate all models with scaled data\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train_scaled_dense, y_train)\n",
    "    y_pred = model.predict(X_test_scaled_dense)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    results[name] = accuracy\n",
    "    print(f\"SCALED MODEL EVAL (DENSE): {name}: {accuracy:.3f}\")\n",
    "best_model = max(results, key=results.get)\n",
    "\n",
    "# Store highest score\n",
    "if results[best_model] > highest_score[\"score\"]:\n",
    "       highest_score[\"score\"] = results[best_model]\n",
    "       highest_score[\"model\"] = best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "773a5c39-c0d9-44eb-8fff-d4d9e2d0d0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA to capture 95% variance\n",
    "pca=PCA(n_components=0.95)\n",
    "X_train_pca = pca.fit_transform(X_train_scaled_dense)\n",
    "X_test_pca = pca.transform(X_test_scaled_dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9c69420d-1411-47cf-b6e6-9235e630d455",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA APPLIED MODEL EVAL RandomForest: 0.507\n",
      "PCA APPLIED MODEL EVAL LogisticRegression: 0.547\n",
      "PCA APPLIED MODEL EVAL SVM: 0.602\n",
      "PCA APPLIED MODEL EVAL KNN: 0.507\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate all models with scaled + PCA data\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train_pca, y_train)\n",
    "    y_pred = model.predict(X_test_pca)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    results[name] = accuracy\n",
    "    print(f\"PCA APPLIED MODEL EVAL {name}: {accuracy:.3f}\")\n",
    "    \n",
    "# Store highest score\n",
    "if results[best_model] > highest_score[\"score\"]:\n",
    "       highest_score[\"score\"] = results[best_model]\n",
    "       highest_score[\"model\"] = best_model\n",
    "best_model = max(results, key=results.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "91605325-41d6-4f89-8453-1f55cb1f2314",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === EXPERIMENT: Optimizing LogisticRegression C parameter ===\n",
    "# After finding LogisticRegression was initially best, tested different regularization strengths\n",
    "# RESULT: Default C=1.0 was optimal for 100 features, but with 600 features, C=0.1 performed better\n",
    "# This led to discovering that more text features (600 vs 100) was more impactful than C tuning\n",
    "lr_models = {\n",
    "    'LR_C=0.1': LogisticRegression(C=0.1, random_state=42, max_iter=10000),\n",
    "    'LR_C=1.0': LogisticRegression(C=1.0, random_state=42, max_iter=10000), \n",
    "    'LR_C=10.0': LogisticRegression(C=10.0, random_state=42, max_iter=10000),\n",
    "}\n",
    "\n",
    "for name, model in lr_models.items():\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    accuracy = accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0333a6bd-3331-44e6-af63-0c043fa819c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best RAW Model: SVM with accuracy: 0.602\n"
     ]
    }
   ],
   "source": [
    "print(f\"Best RAW Model: {best_model} with accuracy: {results[best_model]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1e75fecf-49f7-4671-b877-68a5f4e4a225",
   "metadata": {},
   "outputs": [],
   "source": [
    "# THOUGHT PROCESS AND EVAL:\n",
    "# Grid search with cross-validation and multiple C values worsened score and same with linearSVC\n",
    "# optimised second best model (lr) with best .54 lower than svm\n",
    "# optimised nbm model scored second best optimised at 5.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4144fc45-fcfb-4d18-8366-cfedd3de2d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LEFT TO TRY\n",
    "#pca variance tuning\n",
    "#only options: stick to 60% with svm AND test with only numeric values\n",
    "# use nbm model (both: convert to positive and try with numeric only)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
